"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[314],{6377:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"development/llm-client-architecture","title":"LLM Client Architecture and Recent Improvements","description":"Overview","source":"@site/docs/development/llm-client-architecture.md","sourceDirName":"development","slug":"/development/llm-client-architecture","permalink":"/libriscribe2/docs/development/llm-client-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/guerra2fernando/libriscribe2/tree/main/docs/docs/development/llm-client-architecture.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Installation for Developers","permalink":"/libriscribe2/docs/development/installation"},"next":{"title":"Timestamp Standards in LibriScribe2","permalink":"/libriscribe2/docs/development/timestamp-standards"}}');var r=i(4848),l=i(8453);const s={},o="LLM Client Architecture and Recent Improvements",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Recent Code Improvement",id:"recent-code-improvement",level:2},{value:"Exception Handling Enhancement",id:"exception-handling-enhancement",level:3},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Key Features",id:"key-features",level:3},{value:"Provider Implementations",id:"provider-implementations",level:2},{value:"OpenAI Provider",id:"openai-provider",level:3},{value:"Mock Provider",id:"mock-provider",level:3},{value:"Content Filtering System",id:"content-filtering-system",level:2},{value:"Detection Mechanisms",id:"detection-mechanisms",level:3},{value:"Fallback Strategy",id:"fallback-strategy",level:3},{value:"Prompt Simplification",id:"prompt-simplification",level:3},{value:"Model Configuration System",id:"model-configuration-system",level:2},{value:"Per-Prompt-Type Models",id:"per-prompt-type-models",level:3},{value:"Dynamic Model Selection",id:"dynamic-model-selection",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:2},{value:"Timing and Metrics",id:"timing-and-metrics",level:3},{value:"Resource Management",id:"resource-management",level:3},{value:"LiteLLM Integration",id:"litellm-integration",level:2},{value:"Metadata Tagging",id:"metadata-tagging",level:3},{value:"Proxy Configuration",id:"proxy-configuration",level:3},{value:"Error Recovery Patterns",id:"error-recovery-patterns",level:2},{value:"Hierarchical Fallback",id:"hierarchical-fallback",level:3},{value:"Exception Hierarchy",id:"exception-hierarchy",level:3},{value:"Testing Strategy",id:"testing-strategy",level:2},{value:"Mock Integration",id:"mock-integration",level:3},{value:"Deterministic Testing",id:"deterministic-testing",level:3},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"Input Sanitization",id:"input-sanitization",level:3},{value:"API Key Protection",id:"api-key-protection",level:3},{value:"Content Filtering",id:"content-filtering",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Model Selection",id:"1-model-selection",level:3},{value:"2. Error Handling",id:"2-error-handling",level:3},{value:"3. Resource Management",id:"3-resource-management",level:3},{value:"4. Performance Optimization",id:"4-performance-optimization",level:3},{value:"5. Testing",id:"5-testing",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Planned Features",id:"planned-features",level:3},{value:"Migration Path",id:"migration-path",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"llm-client-architecture-and-recent-improvements",children:"LLM Client Architecture and Recent Improvements"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"The LibriScribe2 LLM Client has been significantly enhanced to provide robust, production-ready AI integration with comprehensive error handling, content filtering fallback, and multi-provider support. This document outlines the architecture, recent improvements, and best practices."}),"\n",(0,r.jsx)(n.h2,{id:"recent-code-improvement",children:"Recent Code Improvement"}),"\n",(0,r.jsx)(n.h3,{id:"exception-handling-enhancement",children:"Exception Handling Enhancement"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Change Made:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Before (line 380)\nexcept:\n    pass\n\n# After (line 380)\nexcept (json.JSONDecodeError, KeyError, TypeError):\n    pass\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Impact:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Improved Error Specificity"}),": Now catches only specific JSON-related exceptions instead of all exceptions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Better Debugging"}),": Allows unexpected exceptions to propagate for proper error handling"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Code Quality"}),": Follows Python best practices for exception handling"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Maintainability"}),": Makes the code more explicit about what errors are expected"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Context:"}),"\nThis change occurs in the ",(0,r.jsx)(n.code,{children:"_create_simplified_prompt()"})," method when attempting to parse JSON content from prompts. The method tries to extract structured data from prompts that contain JSON, and if parsing fails, it gracefully falls back to a simpler prompt format."]}),"\n",(0,r.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,r.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[LLMClient] --\x3e B[Provider Selection]\n    B --\x3e C[OpenAI Provider]\n    B --\x3e D[Mock Provider]\n\n    C --\x3e E[Content Filtering Analysis]\n    C --\x3e F[LiteLLM Integration]\n    C --\x3e G[Error Recovery]\n\n    D --\x3e H[MockLLMClient]\n    H --\x3e I[Multi-Language Support]\n    H --\x3e J[Deterministic Responses]\n\n    A --\x3e K[Content Generation]\n    A --\x3e L[Streaming Generation]\n    A --\x3e M[Performance Monitoring]\n\n    K --\x3e N[Fallback Mechanisms]\n    N --\x3e O[Primary Prompt]\n    N --\x3e P[Simplified Prompt]\n    N --\x3e Q[Content Filtering Recovery]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Multi-Provider Architecture"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"OpenAI integration with LiteLLM proxy support"}),"\n",(0,r.jsx)(n.li,{children:"Mock provider for testing and development"}),"\n",(0,r.jsx)(n.li,{children:"Extensible design for additional providers"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advanced Error Handling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Specific exception types for different error conditions"}),"\n",(0,r.jsx)(n.li,{children:"Comprehensive error context and logging"}),"\n",(0,r.jsx)(n.li,{children:"Automatic retry mechanisms with exponential backoff"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Content Filtering Resilience"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Automatic detection of content filtering triggers"}),"\n",(0,r.jsx)(n.li,{children:"Multi-level fallback system"}),"\n",(0,r.jsx)(n.li,{children:"Prompt simplification algorithms"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Performance Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Async/await patterns throughout"}),"\n",(0,r.jsx)(n.li,{children:"Connection pooling and session management"}),"\n",(0,r.jsx)(n.li,{children:"Timing and metrics collection"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"provider-implementations",children:"Provider Implementations"}),"\n",(0,r.jsx)(n.h3,{id:"openai-provider",children:"OpenAI Provider"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Full OpenAI API compatibility"}),"\n",(0,r.jsx)(n.li,{children:"LiteLLM proxy integration with metadata tagging"}),"\n",(0,r.jsx)(n.li,{children:"Comprehensive request/response validation"}),"\n",(0,r.jsx)(n.li,{children:"Content filtering detection and recovery"}),"\n",(0,r.jsx)(n.li,{children:"Timeout and retry handling"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Configuration:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'client = LLMClient(\n    provider="openai",\n    model_config={\n        "creative": "gpt-4o",\n        "general": "gpt-4o-mini",\n        "analysis": "gpt-4o"\n    },\n    timeout=300.0,\n    environment="production",\n    project_name="my-book-project"\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Error Handling:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Network connectivity issues"}),"\n",(0,r.jsx)(n.li,{children:"API rate limiting"}),"\n",(0,r.jsx)(n.li,{children:"Content filtering responses"}),"\n",(0,r.jsx)(n.li,{children:"Invalid response formats"}),"\n",(0,r.jsx)(n.li,{children:"Timeout conditions"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"mock-provider",children:"Mock Provider"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Deterministic responses for testing"}),"\n",(0,r.jsx)(n.li,{children:"Multi-language content generation (English, French, Spanish)"}),"\n",(0,r.jsx)(n.li,{children:"Configurable content length and structure"}),"\n",(0,r.jsx)(n.li,{children:"JSON response simulation"}),"\n",(0,r.jsx)(n.li,{children:"No external dependencies or API costs"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Configuration:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'mock_client = MockLLMClient(\n    model_config={"creative": "mock-gpt-4"},\n    mock_config={\n        "scene_length": {"min_words": 200, "max_words": 400},\n        "chapter_length": {"min_words": 1000, "max_words": 2000}\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"content-filtering-system",children:"Content Filtering System"}),"\n",(0,r.jsx)(n.h3,{id:"detection-mechanisms",children:"Detection Mechanisms"}),"\n",(0,r.jsx)(n.p,{children:"The LLM client includes sophisticated content filtering detection:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def _analyze_content_filtering_triggers(self, prompt: Any) -> list[str]:\n    """Analyze prompt for potential content filtering triggers."""\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Detected Patterns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Unicode escape sequences"}),"\n",(0,r.jsx)(n.li,{children:"HTML-like tags"}),"\n",(0,r.jsx)(n.li,{children:"JavaScript injection attempts"}),"\n",(0,r.jsx)(n.li,{children:"URLs and email patterns"}),"\n",(0,r.jsx)(n.li,{children:"IP addresses"}),"\n",(0,r.jsx)(n.li,{children:"Non-ASCII characters"}),"\n",(0,r.jsx)(n.li,{children:"Excessive length or repetition"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"fallback-strategy",children:"Fallback Strategy"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Primary Prompt"}),": Original user prompt"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fallback Prompt"}),": User-provided alternative"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simplified Prompt"}),": Auto-generated simplified version"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error Handling"}),": Graceful failure with context"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"async def generate_content_with_content_filtering_fallback(\n    self,\n    primary_prompt: str,\n    fallback_prompt: str | None = None,\n    max_retries: int = 2,\n    **kwargs: Any,\n) -> str | None:\n"})}),"\n",(0,r.jsx)(n.h3,{id:"prompt-simplification",children:"Prompt Simplification"}),"\n",(0,r.jsx)(n.p,{children:"The system automatically creates simplified prompts when content filtering is detected:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def _create_simplified_prompt(self, original_prompt: str, prompt_type: str) -> str:\n    """Create a simplified version of the prompt to avoid content filtering."""\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Simplification Strategies:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Extract essential information from JSON structures"}),"\n",(0,r.jsx)(n.li,{children:"Remove potentially problematic content"}),"\n",(0,r.jsx)(n.li,{children:"Reduce prompt length"}),"\n",(0,r.jsx)(n.li,{children:"Use safer vocabulary and phrasing"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"model-configuration-system",children:"Model Configuration System"}),"\n",(0,r.jsx)(n.h3,{id:"per-prompt-type-models",children:"Per-Prompt-Type Models"}),"\n",(0,r.jsx)(n.p,{children:"The client supports different models for different types of content:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'DEFAULT_MODEL_CONFIG = {\n    "general": "gpt-4o-mini",           # General purpose, cost-effective\n    "creative": "gpt-4o",               # Creative writing, higher quality\n    "analysis": "gpt-4o",               # Text analysis, reasoning\n    "title_generation": "gpt-4o-mini",  # Simple tasks\n    "character_generation": "gpt-4o",   # Complex character development\n    "outline_generation": "gpt-4o",     # Story structure\n    "chapter_writing": "gpt-4o",        # Long-form content\n    "worldbuilding": "gpt-4o",          # Complex world creation\n    "review": "gpt-4o",                 # Content evaluation\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"dynamic-model-selection",children:"Dynamic Model Selection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def get_model_for_prompt_type(self, prompt_type: str) -> str:\n    """Gets the specific model for a given prompt type, falling back to default."""\n    return self.model_config.get(prompt_type, self.model_config.get("default", "gpt-4o-mini"))\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,r.jsx)(n.h3,{id:"timing-and-metrics",children:"Timing and Metrics"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'async def generate_content_with_timing(self, prompt: str, **kwargs: Any) -> tuple[str, float]:\n    """Generate content with timing information."""\n    start_time = time.time()\n    result = await self.generate_content(prompt, **kwargs)\n    duration = time.time() - start_time\n    return result, duration\n'})}),"\n",(0,r.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'@asynccontextmanager\nasync def client_session(self):\n    """Async context manager for LLM client sessions."""\n    try:\n        self.logger.info("Starting LLM client session")\n        yield self\n    finally:\n        self.logger.info("LLM client session completed")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"litellm-integration",children:"LiteLLM Integration"}),"\n",(0,r.jsx)(n.h3,{id:"metadata-tagging",children:"Metadata Tagging"}),"\n",(0,r.jsx)(n.p,{children:"The client automatically includes metadata in requests:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'headers = {\n    "Authorization": f"Bearer {api_key}",\n    "Content-Type": "application/json"\n}\n\n# Add LiteLLM tags\nif self.project_name:\n    headers["X-Litellm-Tag-Project"] = self.project_name\nif self.environment:\n    headers["X-Litellm-Tag-Environment"] = self.environment\nif self.user:\n    headers["X-Litellm-Tag-User"] = self.user\n'})}),"\n",(0,r.jsx)(n.h3,{id:"proxy-configuration",children:"Proxy Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Environment variables for LiteLLM\nOPENAI_API_KEY=your-api-key\nOPENAI_BASE_URL=https://your-litellm-proxy.com/v1\n"})}),"\n",(0,r.jsx)(n.h2,{id:"error-recovery-patterns",children:"Error Recovery Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"hierarchical-fallback",children:"Hierarchical Fallback"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Primary Generation"}),": Use specified model and prompt"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Fallback"}),": Try fallback model if primary fails"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prompt Fallback"}),": Use simplified prompt if content filtering detected"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Provider Fallback"}),": Switch to mock provider if all else fails"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exception-hierarchy",children:"Exception Hierarchy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'try:\n    content = await client.generate_content(prompt)\nexcept LLMClientError as e:\n    if "timeout" in str(e).lower():\n        # Handle timeout with retry\n        pass\n    elif "content filtering" in str(e).lower():\n        # Handle content filtering with fallback\n        pass\n    elif "rate limit" in str(e).lower():\n        # Handle rate limiting with backoff\n        pass\n    else:\n        # Handle other errors\n        pass\n'})}),"\n",(0,r.jsx)(n.h2,{id:"testing-strategy",children:"Testing Strategy"}),"\n",(0,r.jsx)(n.h3,{id:"mock-integration",children:"Mock Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Development/Testing\nclient = LLMClient(provider="mock")\n\n# Production\nclient = LLMClient(provider="openai")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"deterministic-testing",children:"Deterministic Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'mock_client = MockLLMClient()\ncontent = mock_client.generate_content("test prompt", "character")\n# Always returns consistent character data for testing\n'})}),"\n",(0,r.jsx)(n.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'async def benchmark_generation():\n    client = LLMClient("openai")\n\n    start_time = time.time()\n    tasks = [\n        client.generate_content(f"Test prompt {i}", "general")\n        for i in range(10)\n    ]\n    results = await asyncio.gather(*tasks)\n    duration = time.time() - start_time\n\n    print(f"Generated {len(results)} responses in {duration:.2f}s")\n    print(f"Average: {duration/len(results):.2f}s per request")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"input-sanitization",children:"Input Sanitization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def validate_prompt(self, prompt: Any) -> bool:\n    """Validate prompt using Python 3.12 features."""\n    if not isinstance(prompt, str):\n        return False\n    if not prompt.strip():\n        return False\n    if len(prompt) > DEFAULT_MAX_TOKENS:\n        return False\n    return True\n'})}),"\n",(0,r.jsx)(n.h3,{id:"api-key-protection",children:"API Key Protection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Safe logging with API key truncation\nsafe_headers = headers.copy()\nif "Authorization" in safe_headers:\n    auth_value = safe_headers["Authorization"]\n    if auth_value.startswith("Bearer "):\n        api_key = auth_value[7:]\n        truncated_key = api_key[:5] + "..." if len(api_key) > 5 else api_key\n        safe_headers["Authorization"] = f"Bearer {truncated_key}"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"content-filtering",children:"Content Filtering"}),"\n",(0,r.jsx)(n.p,{children:"The system proactively analyzes prompts for potentially problematic content and applies appropriate safeguards."}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"1-model-selection",children:"1. Model Selection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Use cost-effective models for simple tasks\nmodel_config = {\n    "simple_tasks": "gpt-4o-mini",\n    "complex_tasks": "gpt-4o"\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-error-handling",children:"2. Error Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Always handle LLMClientError with context\ntry:\n    content = await client.generate_content(prompt)\nexcept LLMClientError as e:\n    logger.error(f"Generation failed: {e}")\n    logger.error(f"Provider: {e.provider}")\n    logger.error(f"Context: {e.context}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-resource-management",children:"3. Resource Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Use context managers for proper cleanup\nasync with LLMClient("openai") as client:\n    content = await client.generate_content(prompt)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"4-performance-optimization",children:"4. Performance Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Use appropriate timeouts\nclient = LLMClient("openai", timeout=300.0)\n\n# Monitor performance\ncontent, duration = await client.generate_content_with_timing(prompt)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"5-testing",children:"5. Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Use mock provider for tests\n@pytest.fixture\ndef mock_llm_client():\n    return LLMClient("mock")\n\ndef test_content_generation(mock_llm_client):\n    content = await mock_llm_client.generate_content("test")\n    assert content is not None\n'})}),"\n",(0,r.jsx)(n.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,r.jsx)(n.h3,{id:"planned-features",children:"Planned Features"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Additional Providers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Anthropic Claude integration"}),"\n",(0,r.jsx)(n.li,{children:"Google Gemini support"}),"\n",(0,r.jsx)(n.li,{children:"Local model support"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advanced Caching"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Response caching for repeated prompts"}),"\n",(0,r.jsx)(n.li,{children:"Intelligent cache invalidation"}),"\n",(0,r.jsx)(n.li,{children:"Distributed caching support"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Enhanced Monitoring"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time performance metrics"}),"\n",(0,r.jsx)(n.li,{children:"Cost tracking and optimization"}),"\n",(0,r.jsx)(n.li,{children:"Usage analytics"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Improved Fallback"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Machine learning-based prompt optimization"}),"\n",(0,r.jsx)(n.li,{children:"Dynamic model selection based on performance"}),"\n",(0,r.jsx)(n.li,{children:"Adaptive retry strategies"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"migration-path",children:"Migration Path"}),"\n",(0,r.jsx)(n.p,{children:"The current architecture is designed to support these enhancements without breaking changes to the public API."}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"The LibriScribe2 LLM Client provides a robust, production-ready foundation for AI integration with comprehensive error handling, content filtering resilience, and performance optimization. The recent exception handling improvement exemplifies the ongoing commitment to code quality and maintainability."}),"\n",(0,r.jsx)(n.p,{children:"The architecture supports both development and production use cases through its dual-provider system, extensive configuration options, and comprehensive monitoring capabilities."})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var t=i(6540);const r={},l=t.createContext(r);function s(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(l.Provider,{value:n},e.children)}}}]);