"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[36],{991:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"api/ai-mock-system","title":"AI Mock System Documentation","description":"Overview","source":"@site/docs/api/ai-mock-system.md","sourceDirName":"api","slug":"/api/ai-mock-system","permalink":"/libriscribe2/docs/api/ai-mock-system","draft":false,"unlisted":false,"editUrl":"https://github.com/guerra2fernando/libriscribe2/tree/main/docs/docs/api/ai-mock-system.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"AI Mock System - Function Signatures","permalink":"/libriscribe2/docs/ai_mock_system_signatures"},"next":{"title":"LibriScribe2 CLI API Documentation","permalink":"/libriscribe2/docs/api/cli-api"}}');var o=s(4848),r=s(8453);const i={},a="AI Mock System Documentation",c={},l=[{value:"Overview",id:"overview",level:2},{value:"\ud83d\udd11 Key Features",id:"-key-features",level:2},{value:"\ud83d\ude80 Quick Start",id:"-quick-start",level:2},{value:"1. Environment Setup",id:"1-environment-setup",level:3},{value:"2. Basic Usage",id:"2-basic-usage",level:3},{value:"\ud83d\udd04 How It Works",id:"-how-it-works",level:2},{value:"Automatic Mode Detection",id:"automatic-mode-detection",level:3},{value:"Real AI Integration",id:"real-ai-integration",level:3},{value:"Mock Response Generation",id:"mock-response-generation",level:3},{value:"\ud83d\udcf9 Live Response Recording",id:"-live-response-recording",level:2},{value:"Automatic Recording",id:"automatic-recording",level:3},{value:"Bulk Recording",id:"bulk-recording",level:3},{value:"Recording Results",id:"recording-results",level:3},{value:"\ud83c\udfad Mock Scenarios",id:"-mock-scenarios",level:2},{value:"Scenario Response Examples",id:"scenario-response-examples",level:3},{value:"\u2699\ufe0f Configuration",id:"\ufe0f-configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:3},{value:"LiteLLM Proxy Setup",id:"litellm-proxy-setup",level:3},{value:"Validation Config Integration",id:"validation-config-integration",level:3},{value:"\ud83e\uddea Testing Integration",id:"-testing-integration",level:2},{value:"Test Framework Usage",id:"test-framework-usage",level:3},{value:"Validator Integration",id:"validator-integration",level:3},{value:"\ud83d\udcca Monitoring and Statistics",id:"-monitoring-and-statistics",level:2},{value:"Usage Statistics",id:"usage-statistics",level:3},{value:"Cost Tracking",id:"cost-tracking",level:3},{value:"Coverage Reporting",id:"coverage-reporting",level:3},{value:"\ud83d\udd27 Advanced Usage",id:"-advanced-usage",level:2},{value:"Custom Mock Responses",id:"custom-mock-responses",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"\ud83d\ude80 Best Practices",id:"-best-practices",level:2},{value:"1. Environment Management",id:"1-environment-management",level:3},{value:"2. Cost Management",id:"2-cost-management",level:3},{value:"3. Recording Strategy",id:"3-recording-strategy",level:3},{value:"4. Testing Strategy",id:"4-testing-strategy",level:3},{value:"\ud83d\udd0d Troubleshooting",id:"-troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debug Mode",id:"debug-mode",level:3},{value:"\ud83d\udcda Examples",id:"-examples",level:2},{value:"\ud83c\udfaf Summary",id:"-summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"ai-mock-system-documentation",children:"AI Mock System Documentation"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:["The AI Mock System provides seamless switching between real AI and mock responses for the LibriScribe validation system. It uses the ",(0,o.jsx)(n.strong,{children:"OpenAI SDK"})," with ",(0,o.jsx)(n.strong,{children:"LiteLLM configured via .env"})," (transparent to the service) and ",(0,o.jsx)(n.strong,{children:"API key-based switching"})," for automatic mode detection."]}),"\n",(0,o.jsx)(n.h2,{id:"-key-features",children:"\ud83d\udd11 Key Features"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenAI SDK Integration"}),": Uses standard OpenAI SDK with LiteLLM proxy configured via environment variables"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"API Key-Based Switching"}),": Empty ",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," = mock mode, set key = real AI mode"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Live Response Recording"}),": Automatically records real AI responses for future mock use"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transparent Operation"}),": Same code works in both mock and real AI modes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cost Tracking"}),": Monitors token usage and API costs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scenario Testing"}),": Comprehensive testing with different response scenarios"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"-quick-start",children:"\ud83d\ude80 Quick Start"}),"\n",(0,o.jsx)(n.h3,{id:"1-environment-setup",children:"1. Environment Setup"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Mock Mode (Development/Testing)\n# Leave OPENAI_API_KEY empty or unset\nunset OPENAI_API_KEY\n\n# Real AI Mode (Production/Live Recording)\nexport OPENAI_API_KEY="your-openai-api-key"\n\n# Optional: Configure LiteLLM Proxy\nexport OPENAI_BASE_URL="https://your-litellm-proxy.com/v1"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-basic-usage",children:"2. Basic Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from src.libriscribe2.validation.ai_mock import AIMockManager\n\n# Create manager (automatically detects mode based on API key)\nmock_manager = AIMockManager()\n\n# Get AI response (works in both mock and real modes)\nresponse = await mock_manager.get_ai_response(\n    prompt="Analyze this chapter for content quality",\n    validator_id="content_validator",\n    content_type="chapter",\n    model="gpt-4"\n)\n\n# Process response (same code for both modes)\nprint(f"Tokens used: {response.tokens_used}")\nprint(f"Cost: ${response.cost:.4f}")\nprint(f"Content: {response.content}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-how-it-works",children:"\ud83d\udd04 How It Works"}),"\n",(0,o.jsx)(n.h3,{id:"automatic-mode-detection",children:"Automatic Mode Detection"}),"\n",(0,o.jsxs)(n.p,{children:["The system automatically determines whether to use mock or real AI based on the ",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," environment variable:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class AIMockManager:\n    def __init__(self):\n        self.openai_api_key = os.getenv("OPENAI_API_KEY", "")\n        self.use_mock_mode = not bool(self.openai_api_key.strip())\n\n        if not self.use_mock_mode:\n            # Initialize OpenAI client with LiteLLM proxy\n            self.openai_client = AsyncOpenAI(\n                api_key=self.openai_api_key,\n                base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")\n            )\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-ai-integration",children:"Real AI Integration"}),"\n",(0,o.jsxs)(n.p,{children:["When ",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," is set, the system uses the OpenAI SDK:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'async def _call_real_ai(self, prompt, validator_id, content_type, model):\n    response = await self.openai_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                "role": "system",\n                "content": f"You are a {validator_id} for libriscribe2. Respond with valid JSON."\n            },\n            {\n                "role": "user",\n                "content": prompt\n            }\n        ],\n        temperature=0.1,\n        max_tokens=2000\n    )\n\n    # Automatically record for future mock use\n    await self._record_interaction(prompt, response, validator_id, content_type)\n\n    return response\n'})}),"\n",(0,o.jsx)(n.h3,{id:"mock-response-generation",children:"Mock Response Generation"}),"\n",(0,o.jsxs)(n.p,{children:["When ",(0,o.jsx)(n.code,{children:"OPENAI_API_KEY"})," is empty, the system generates mock responses:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"async def _get_mock_response(self, prompt, validator_id, content_type, scenario):\n    # First, try to find recorded interaction\n    request_hash = self._generate_request_hash(prompt, validator_id, content_type)\n    if request_hash in self.recorded_interactions:\n        return self.recorded_interactions[request_hash].response\n\n    # Otherwise, generate scenario-based response\n    return await self._generate_scenario_response(scenario)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"-live-response-recording",children:"\ud83d\udcf9 Live Response Recording"}),"\n",(0,o.jsx)(n.h3,{id:"automatic-recording",children:"Automatic Recording"}),"\n",(0,o.jsx)(n.p,{children:"When using real AI, responses are automatically recorded for future mock use:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Real AI call automatically records response\nresponse = await mock_manager.get_ai_response(\n    prompt="Validate this content",\n    validator_id="content_validator",\n    content_type="chapter",\n    model="gpt-4"\n)\n# Response is saved to .libriscribe2/mock_data/recorded_interactions.json\n'})}),"\n",(0,o.jsx)(n.h3,{id:"bulk-recording",children:"Bulk Recording"}),"\n",(0,o.jsx)(n.p,{children:"You can populate mock data from multiple live responses:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Define prompts to record\nprompts = [\n    {\n        "prompt": "Analyze chapter for tone consistency",\n        "validator_id": "content_validator",\n        "content_type": "chapter",\n        "expected_scenario": "success"\n    },\n    {\n        "prompt": "Check manuscript publishing standards",\n        "validator_id": "publishing_standards_validator",\n        "content_type": "manuscript",\n        "expected_scenario": "success"\n    }\n]\n\n# Record all prompts\nresults = await mock_manager.populate_mock_mappings_from_live(\n    prompts=prompts,\n    model="gpt-4"\n)\n\nprint(f"Recorded {results[\'successful_recordings\']} interactions")\nprint(f"Total cost: ${sum(r[\'cost\'] for r in results[\'recordings\']):.4f}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"recording-results",children:"Recording Results"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "total_prompts": 2,\n  "successful_recordings": 2,\n  "failed_recordings": 0,\n  "recordings": [\n    {\n      "prompt": "Analyze chapter for tone consistency",\n      "validator_id": "content_validator",\n      "content_type": "chapter",\n      "tokens_used": 245,\n      "cost": 0.00735,\n      "expected_scenario": "success",\n      "recorded_at": "2024-01-15T10:30:00"\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-mock-scenarios",children:"\ud83c\udfad Mock Scenarios"}),"\n",(0,o.jsx)(n.p,{children:"The system supports comprehensive scenario testing:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from src.libriscribe2.validation.ai_mock import MockScenario\n\nscenarios = [\n    MockScenario.SUCCESS,           # Normal successful validation\n    MockScenario.HIGH_QUALITY,      # High quality content (>90% score)\n    MockScenario.LOW_QUALITY,       # Low quality content (<70% score)\n    MockScenario.FAILURE,           # Complete validation failure\n    MockScenario.TIMEOUT,           # AI timeout simulation\n    MockScenario.RATE_LIMIT,        # Rate limiting simulation\n    MockScenario.INVALID_RESPONSE,  # Malformed JSON response\n    MockScenario.PARTIAL_FAILURE,   # Some checks pass, some fail\n    MockScenario.EDGE_CASE          # Edge cases (empty content, etc.)\n]\n\n# Test specific scenario\nresponse = await mock_manager.get_ai_response(\n    prompt="Test content",\n    validator_id="content_validator",\n    content_type="chapter",\n    scenario=MockScenario.LOW_QUALITY  # Only used in mock mode\n)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"scenario-response-examples",children:"Scenario Response Examples"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"SUCCESS Scenario:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "tone_consistency_score": 85.0,\n  "outline_adherence_score": 90.0,\n  "quality_score": 87.5,\n  "findings": [\n    {\n      "type": "tone_consistency",\n      "severity": "low",\n      "message": "Minor tone variation in chapter 3"\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"LOW_QUALITY Scenario:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "validation_score": 65.0,\n  "quality_score": 62.0,\n  "status": "needs_review",\n  "findings": [\n    {\n      "type": "content_quality",\n      "severity": "high",\n      "message": "Content quality below threshold"\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"FAILURE Scenario:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "error": "Validation failed",\n  "error_code": "VALIDATION_ERROR",\n  "status": "failed"\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"\ufe0f-configuration",children:"\u2699\ufe0f Configuration"}),"\n",(0,o.jsx)(n.h3,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Required for real AI mode\nOPENAI_API_KEY=your-openai-api-key\n\n# Optional: LiteLLM proxy endpoint\nOPENAI_BASE_URL=https://your-litellm-proxy.com/v1\n\n# Optional: Default model\nOPENAI_DEFAULT_MODEL=gpt-4\n"})}),"\n",(0,o.jsx)(n.h3,{id:"litellm-proxy-setup",children:"LiteLLM Proxy Setup"}),"\n",(0,o.jsx)(n.p,{children:"The system works transparently with LiteLLM proxy. Configure your LiteLLM proxy and set the base URL:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# litellm_config.yaml\nmodel_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: openai/gpt-4\n      api_key: your-actual-openai-key\n\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: your-actual-openai-key\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Start LiteLLM proxy\nlitellm --config litellm_config.yaml --port 8000\n\n# Configure LibriScribe to use proxy\nexport OPENAI_API_KEY="your-proxy-auth-key"\nexport OPENAI_BASE_URL="http://localhost:8000/v1"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"validation-config-integration",children:"Validation Config Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from src.libriscribe2.validation import ValidationConfig\n\n# Configuration automatically detects mock/real mode\nconfig = ValidationConfig(\n    project_id="my_project",\n    validator_configs={\n        "content_validator": {\n            "openai_model": "gpt-4",\n            "temperature": 0.1,\n            "max_tokens": 2000\n        }\n    }\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-testing-integration",children:"\ud83e\uddea Testing Integration"}),"\n",(0,o.jsx)(n.h3,{id:"test-framework-usage",children:"Test Framework Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from tests.utils.test_framework import ValidationTestFramework\n\n# Create test framework\nmock_manager = AIMockManager()\ntest_framework = ValidationTestFramework(mock_manager)\n\n# Generate comprehensive test suite\ntest_cases = await test_framework.create_comprehensive_test_suite(\n    validators=["content_validator", "publishing_standards_validator"],\n    content_types=["chapter", "manuscript"]\n)\n\n# Run tests (automatically uses mock mode if no API key)\nresults = await test_framework.run_test_suite(test_cases)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"validator-integration",children:"Validator Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from src.libriscribe2.validation.interfaces import ValidatorBase\n\nclass ContentValidator(ValidatorBase):\n    def __init__(self, mock_manager):\n        super().__init__("content_validator", "Content Validator", "1.0.0")\n        self.mock_manager = mock_manager\n\n    async def validate(self, content, context):\n        # Call AI (automatically uses mock or real based on API key)\n        ai_response = await self.mock_manager.get_ai_response(\n            prompt=f"Analyze content quality: {content}",\n            validator_id=self.validator_id,\n            content_type="chapter",\n            model="gpt-4"\n        )\n\n        # Process response (same code for mock or real)\n        response_data = json.loads(ai_response.content)\n\n        return ValidatorResult(\n            validator_id=self.validator_id,\n            status=ValidationStatus.COMPLETED,\n            findings=self._parse_findings(response_data),\n            metrics={\n                "quality_score": response_data.get("validation_score", 0),\n                "ai_tokens": ai_response.tokens_used,\n                "ai_cost": ai_response.cost\n            }\n        )\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-monitoring-and-statistics",children:"\ud83d\udcca Monitoring and Statistics"}),"\n",(0,o.jsx)(n.h3,{id:"usage-statistics",children:"Usage Statistics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Get comprehensive usage statistics\nstats = mock_manager.get_usage_stats()\n\nprint(f"Mock calls: {stats[\'mock_calls\']}")\nprint(f"Real AI calls: {stats[\'real_calls\']}")\nprint(f"Live recordings: {stats[\'live_recordings\']}")\nprint(f"Total cost: ${sum(r.cost for r in recorded_responses):.4f}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"cost-tracking",children:"Cost Tracking"}),"\n",(0,o.jsx)(n.p,{children:"The system automatically tracks costs for real AI calls:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Cost calculation based on model and tokens\ndef _calculate_cost(self, model: str, tokens_used: int) -> float:\n    pricing = {\n        "gpt-4": 0.03,           # $0.03 per 1K tokens\n        "gpt-4-turbo": 0.01,     # $0.01 per 1K tokens\n        "gpt-3.5-turbo": 0.002,  # $0.002 per 1K tokens\n    }\n    price_per_1k = pricing.get(model, 0.01)\n    return (tokens_used / 1000) * price_per_1k\n'})}),"\n",(0,o.jsx)(n.h3,{id:"coverage-reporting",children:"Coverage Reporting"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from tests.lib.coverage import CoverageReporter\n\ncoverage_reporter = CoverageReporter()\ncoverage_report = await coverage_reporter.generate_coverage_report(test_results)\n\nprint(f"Overall coverage: {coverage_report.overall_coverage.coverage_percentage:.1f}%")\nprint(f"Validators tested: {len(coverage_report.validator_coverage)}")\nprint(f"Scenarios covered: {len(coverage_report.scenario_coverage)}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-advanced-usage",children:"\ud83d\udd27 Advanced Usage"}),"\n",(0,o.jsx)(n.h3,{id:"custom-mock-responses",children:"Custom Mock Responses"}),"\n",(0,o.jsx)(n.p,{children:"You can add custom mock responses for specific prompts:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Add custom mock response\ncustom_response = MockResponse(\n    content=json.dumps({\n        "validation_score": 95.0,\n        "status": "excellent",\n        "custom_field": "custom_value"\n    }),\n    model="gpt-4",\n    tokens_used=150,\n    cost=0.0045,\n    scenario=MockScenario.SUCCESS\n)\n\n# Save to recorded interactions\nawait mock_manager._record_interaction(\n    prompt="Your custom prompt",\n    response=custom_response,\n    validator_id="custom_validator",\n    content_type="custom_type"\n)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Process multiple prompts efficiently\nprompts = [\n    {"prompt": "Validate chapter 1", "validator_id": "content_validator"},\n    {"prompt": "Validate chapter 2", "validator_id": "content_validator"},\n    {"prompt": "Check manuscript", "validator_id": "publishing_validator"}\n]\n\nresponses = []\nfor prompt_config in prompts:\n    response = await mock_manager.get_ai_response(\n        prompt=prompt_config["prompt"],\n        validator_id=prompt_config["validator_id"],\n        content_type="chapter",\n        model="gpt-4"\n    )\n    responses.append(response)\n\ntotal_cost = sum(r.cost for r in responses)\ntotal_tokens = sum(r.tokens_used for r in responses)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'try:\n    response = await mock_manager.get_ai_response(\n        prompt="Your prompt",\n        validator_id="validator_id",\n        content_type="chapter",\n        model="gpt-4"\n    )\nexcept Exception as e:\n    if "rate limit" in str(e).lower():\n        # Handle rate limiting\n        await asyncio.sleep(60)\n        # Retry logic\n    elif "timeout" in str(e).lower():\n        # Handle timeout\n        # Use cached response or fallback\n    else:\n        # Handle other errors\n        logger.error(f"AI call failed: {e}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-best-practices",children:"\ud83d\ude80 Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"1-environment-management",children:"1. Environment Management"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Development\nexport OPENAI_API_KEY=""  # Empty for mock mode\n\n# Testing with live recording\nexport OPENAI_API_KEY="sk-test-key"\nexport OPENAI_BASE_URL="https://api.openai.com/v1"\n\n# Production with LiteLLM\nexport OPENAI_API_KEY="proxy-auth-key"\nexport OPENAI_BASE_URL="https://your-proxy.com/v1"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-cost-management",children:"2. Cost Management"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Monitor costs in real AI mode\nif not mock_manager.use_mock_mode:\n    stats = mock_manager.get_usage_stats()\n    total_cost = sum(r.cost for r in recorded_responses)\n\n    if total_cost > 10.0:  # $10 threshold\n        logger.warning(f"High AI usage cost: ${total_cost:.2f}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-recording-strategy",children:"3. Recording Strategy"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Record comprehensive test data\nrecording_prompts = [\n    # Success cases\n    {"prompt": "High quality content", "expected_scenario": "high_quality"},\n    {"prompt": "Normal content", "expected_scenario": "success"},\n\n    # Failure cases\n    {"prompt": "Poor quality content", "expected_scenario": "low_quality"},\n    {"prompt": "Invalid content", "expected_scenario": "failure"},\n\n    # Edge cases\n    {"prompt": "", "expected_scenario": "edge_case"},\n    {"prompt": "A" * 10000, "expected_scenario": "edge_case"}\n]\n\n# Record during development phase\nif not mock_manager.use_mock_mode:\n    await mock_manager.populate_mock_mappings_from_live(recording_prompts)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"4-testing-strategy",children:"4. Testing Strategy"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Comprehensive testing approach\nasync def test_validator_with_all_scenarios():\n    scenarios = [MockScenario.SUCCESS, MockScenario.FAILURE, MockScenario.LOW_QUALITY]\n\n    for scenario in scenarios:\n        response = await mock_manager.get_ai_response(\n            prompt="Test content",\n            validator_id="content_validator",\n            content_type="chapter",\n            scenario=scenario\n        )\n\n        # Verify response matches scenario expectations\n        assert_scenario_response(response, scenario)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-troubleshooting",children:"\ud83d\udd0d Troubleshooting"}),"\n",(0,o.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:'1. "OpenAI client not initialized"'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Solution: Set API key\nexport OPENAI_API_KEY="your-key-here"\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:'2. "Mock mode active - cannot record"'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Solution: Provide API key for recording\nexport OPENAI_API_KEY="your-key-here"\npython record_responses.py\nunset OPENAI_API_KEY  # Return to mock mode\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:'3. "No recorded interactions found"'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Solution: Record some interactions first\nawait mock_manager.populate_mock_mappings_from_live(prompts)\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"4. Rate limiting errors"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Solution: Add delays between calls\nawait asyncio.sleep(1.0)  # 1 second delay\n"})}),"\n",(0,o.jsx)(n.h3,{id:"debug-mode",children:"Debug Mode"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Enable detailed logging\nmock_manager = AIMockManager()\nresponse = await mock_manager.get_ai_response(...)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"-examples",children:"\ud83d\udcda Examples"}),"\n",(0,o.jsx)(n.p,{children:"See the complete examples in:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"examples/ai_mock_system_usage.py"})," - Comprehensive usage demonstration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"examples/ai_testing_best_practices.py"})," - Testing best practices"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"docs/ai_testing_best_practices.md"})," - Detailed best practices guide"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"-summary",children:"\ud83c\udfaf Summary"}),"\n",(0,o.jsx)(n.p,{children:"The AI Mock System provides:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Seamless Integration"}),": Uses OpenAI SDK with LiteLLM proxy (transparent)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Automatic Switching"}),": API key presence determines mock vs real mode"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Live Recording"}),": Real responses automatically recorded for mock use"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cost Efficiency"}),": Free, fast mock responses for development/testing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Comprehensive Testing"}),": Multiple scenarios and edge cases covered"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Production Ready"}),": Robust error handling and monitoring"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This system enables efficient development and testing while maintaining compatibility with production AI services."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>a});var t=s(6540);const o={},r=t.createContext(o);function i(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);