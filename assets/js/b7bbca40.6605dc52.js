"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[350],{5128:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"api/llm-client-api","title":"LLM Client API Documentation","description":"Overview","source":"@site/docs/api/llm-client-api.md","sourceDirName":"api","slug":"/api/llm-client-api","permalink":"/libriscribe2/docs/api/llm-client-api","draft":false,"unlisted":false,"editUrl":"https://github.com/guerra2fernando/libriscribe2/tree/main/docs/docs/api/llm-client-api.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"concept-generator-api","permalink":"/libriscribe2/docs/api/concept-generator-api"},"next":{"title":"LLM Client - Function and Class Signatures","permalink":"/libriscribe2/docs/api/llm-client-signatures"}}');var i=t(4848),l=t(8453);const s={},o="LLM Client API Documentation",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Class Definition",id:"class-definition",level:2},{value:"Constructor Parameters",id:"constructor-parameters",level:2},{value:"Core Methods",id:"core-methods",level:2},{value:"generate_content()",id:"generate_content",level:3},{value:"generate_streaming_content()",id:"generate_streaming_content",level:3},{value:"generate_content_with_content_filtering_fallback()",id:"generate_content_with_content_filtering_fallback",level:3},{value:"generate_content_with_timing()",id:"generate_content_with_timing",level:3},{value:"generate_content_with_fallback()",id:"generate_content_with_fallback",level:3},{value:"Utility Methods",id:"utility-methods",level:2},{value:"get_model_for_prompt_type()",id:"get_model_for_prompt_type",level:3},{value:"validate_prompt()",id:"validate_prompt",level:3},{value:"get_client_config()",id:"get_client_config",level:3},{value:"Context Manager Support",id:"context-manager-support",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"LLMClientError",id:"llmclienterror",level:3},{value:"Model Configuration",id:"model-configuration",level:2},{value:"LiteLLM Integration",id:"litellm-integration",level:2},{value:"Content Filtering Analysis",id:"content-filtering-analysis",level:2},{value:"Provider-Specific Implementation",id:"provider-specific-implementation",level:2},{value:"OpenAI Provider",id:"openai-provider",level:3},{value:"Mock Provider",id:"mock-provider",level:3},{value:"Usage Examples",id:"usage-examples",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Advanced Usage with Error Handling",id:"advanced-usage-with-error-handling",level:3},{value:"Streaming Generation",id:"streaming-generation",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Context Manager Usage",id:"context-manager-usage",level:3},{value:"Configuration Integration",id:"configuration-integration",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Thread Safety",id:"thread-safety",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Migration Guide",id:"migration-guide",level:2},{value:"From Previous Versions",id:"from-previous-versions",level:3},{value:"Updating Code",id:"updating-code",level:3},{value:"See Also",id:"see-also",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llm-client-api-documentation",children:"LLM Client API Documentation"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"LLMClient"})," class provides a unified interface for interacting with Large Language Models (LLMs) in LibriScribe2. It supports multiple providers (OpenAI, mock) with advanced features like content filtering fallback, streaming generation, and comprehensive error handling."]}),"\n",(0,i.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Provider Support"}),": OpenAI and mock providers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Configuration"}),": Per-prompt-type model selection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Filtering Fallback"}),": Automatic retry with simplified prompts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Streaming Generation"}),": Real-time content streaming"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Comprehensive Error Handling"}),": Detailed error context and recovery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiteLLM Integration"}),": Full support for LiteLLM proxy with metadata"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Monitoring"}),": Built-in timing and metrics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Python 3.12 Features"}),": Modern async/await patterns and type hints"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"class-definition",children:"Class Definition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LLMClient:\n    """LLM Client using Python 3.12 features."""\n\n    def __init__(\n        self,\n        provider: str,\n        model_config: dict[str, str] | None = None,\n        timeout: float = DEFAULT_TIMEOUT,\n        environment: str = DEFAULT_ENVIRONMENT,\n        project_name: str = "",\n        user: str | None = None,\n    )\n'})}),"\n",(0,i.jsx)(n.h2,{id:"constructor-parameters",children:"Constructor Parameters"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Default"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"provider"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"str"})}),(0,i.jsx)(n.td,{children:"Required"}),(0,i.jsx)(n.td,{children:'LLM provider ("openai", "mock")'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model_config"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"dict[str, str] | None"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"None"})}),(0,i.jsx)(n.td,{children:"Model mapping for different prompt types"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"timeout"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"float"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"300.0"})}),(0,i.jsx)(n.td,{children:"Request timeout in seconds"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"environment"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"str"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:'"production"'})}),(0,i.jsx)(n.td,{children:"Environment tag for LiteLLM"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"project_name"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"str"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:'""'})}),(0,i.jsx)(n.td,{children:"Project name for LiteLLM metadata"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"user"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"str | None"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"None"})}),(0,i.jsx)(n.td,{children:"User identifier for LiteLLM"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"core-methods",children:"Core Methods"}),"\n",(0,i.jsx)(n.h3,{id:"generate_content",children:"generate_content()"}),"\n",(0,i.jsx)(n.p,{children:"Generate content with comprehensive error handling and model selection."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def generate_content(\n    self,\n    prompt: str,\n    prompt_type: str = "general",\n    temperature: float = DEFAULT_TEMPERATURE,\n    max_tokens: int | None = None,\n    **kwargs: Any,\n) -> str\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt"})," (str): The input prompt text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt_type"}),' (str): Type of prompt for model selection ("general", "creative", "analysis", etc.)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"temperature"})," (float): Sampling temperature (0.0-1.0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_tokens"})," (int | None): Maximum tokens to generate"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional provider-specific parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"str"}),": Generated content"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Raises:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"LLMClientError"}),": On generation failure with detailed context"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'client = LLMClient("openai", model_config={"creative": "gpt-4o", "general": "gpt-4o-mini"})\n\n# Generate creative content\ncontent = await client.generate_content(\n    "Write a fantasy story opening",\n    prompt_type="creative",\n    temperature=0.8\n)\n\n# Generate analytical content\nanalysis = await client.generate_content(\n    "Analyze this text for themes",\n    prompt_type="analysis",\n    temperature=0.3\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"generate_streaming_content",children:"generate_streaming_content()"}),"\n",(0,i.jsx)(n.p,{children:"Generate content with real-time streaming."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"async def generate_streaming_content(\n    self,\n    prompt: str,\n    temperature: float = DEFAULT_TEMPERATURE,\n    max_tokens: int | None = None,\n    **kwargs: Any\n) -> AsyncIterator[str]\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt"})," (str): The input prompt text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"temperature"})," (float): Sampling temperature"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_tokens"})," (int | None): Maximum tokens to generate"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"AsyncIterator[str]"}),": Stream of content chunks"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async for chunk in client.generate_streaming_content("Tell me a story"):\n    print(chunk, end="", flush=True)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"generate_content_with_content_filtering_fallback",children:"generate_content_with_content_filtering_fallback()"}),"\n",(0,i.jsx)(n.p,{children:"Generate content with automatic fallback for content filtering issues."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def generate_content_with_content_filtering_fallback(\n    self,\n    primary_prompt: str,\n    fallback_prompt: str | None = None,\n    prompt_type: str = "general",\n    temperature: float = DEFAULT_TEMPERATURE,\n    max_retries: int = 2,\n    **kwargs: Any,\n) -> str | None\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"primary_prompt"})," (str): Primary prompt to try first"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"fallback_prompt"})," (str | None): Optional fallback prompt"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt_type"})," (str): Type of prompt for model selection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"temperature"})," (float): Sampling temperature"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_retries"})," (int): Maximum retry attempts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"str | None"}),": Generated content or None if all attempts fail"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'content = await client.generate_content_with_content_filtering_fallback(\n    primary_prompt="Complex prompt with potential filtering issues",\n    fallback_prompt="Simple alternative prompt",\n    prompt_type="creative",\n    max_retries=3\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"generate_content_with_timing",children:"generate_content_with_timing()"}),"\n",(0,i.jsx)(n.p,{children:"Generate content with performance timing information."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"async def generate_content_with_timing(\n    self,\n    prompt: str,\n    **kwargs: Any\n) -> tuple[str, float]\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt"})," (str): The input prompt text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"tuple[str, float]"}),": Generated content and execution time in seconds"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'content, duration = await client.generate_content_with_timing("Generate a summary")\nprint(f"Generated {len(content)} characters in {duration:.2f} seconds")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"generate_content_with_fallback",children:"generate_content_with_fallback()"}),"\n",(0,i.jsx)(n.p,{children:"Generate content with fallback prompt support."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"async def generate_content_with_fallback(\n    self,\n    primary_prompt: str,\n    fallback_prompt: str | None = None,\n    **kwargs: Any\n) -> str\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"primary_prompt"})," (str): Primary prompt to try first"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"fallback_prompt"})," (str | None): Fallback prompt if primary fails"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"str"}),": Generated content"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'content = await client.generate_content_with_fallback(\n    primary_prompt="Complex technical prompt",\n    fallback_prompt="Simplified version of the prompt"\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"utility-methods",children:"Utility Methods"}),"\n",(0,i.jsx)(n.h3,{id:"get_model_for_prompt_type",children:"get_model_for_prompt_type()"}),"\n",(0,i.jsx)(n.p,{children:"Get the appropriate model for a specific prompt type."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def get_model_for_prompt_type(self, prompt_type: str) -> str\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt_type"})," (str): The prompt type"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"str"}),": Model name for the prompt type"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'model = client.get_model_for_prompt_type("creative")  # Returns "gpt-4o"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"validate_prompt",children:"validate_prompt()"}),"\n",(0,i.jsx)(n.p,{children:"Validate a prompt before processing."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def validate_prompt(self, prompt: Any) -> bool\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt"})," (Any): Prompt to validate"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"bool"}),": True if prompt is valid"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"if client.validate_prompt(user_input):\n    content = await client.generate_content(user_input)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"get_client_config",children:"get_client_config()"}),"\n",(0,i.jsx)(n.p,{children:"Get current client configuration."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def get_client_config(self) -> dict[str, Any]\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dict[str, Any]"}),": Client configuration dictionary"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"config = client.get_client_config()\nprint(f\"Provider: {config['provider']}\")\nprint(f\"Timeout: {config['timeout']}\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"context-manager-support",children:"Context Manager Support"}),"\n",(0,i.jsx)(n.p,{children:"The LLMClient supports async context manager usage for resource management."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def client_session(self):\n    """Async context manager for LLM client sessions."""\n\nasync def __aenter__(self):\n    """Async context manager entry."""\n\nasync def __aexit__(self, exc_type, _exc_val, _exc_tb):\n    """Async context manager exit."""\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async with LLMClient("openai") as client:\n    content = await client.generate_content("Generate content")\n    # Client automatically cleaned up\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(n.h3,{id:"llmclienterror",children:"LLMClientError"}),"\n",(0,i.jsx)(n.p,{children:"Custom exception class with enhanced error information."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LLMClientError(Exception):\n    """Exception for LLM client errors with improved error messages."""\n\n    def __init__(\n        self,\n        message: str,\n        provider: str,\n        context: dict[str, Any] | None = None\n    ) -> None\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Attributes:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"provider"})," (str): The LLM provider that caused the error"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"context"})," (dict): Additional error context information"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'try:\n    content = await client.generate_content("Invalid prompt")\nexcept LLMClientError as e:\n    print(f"Provider: {e.provider}")\n    print(f"Context: {e.context}")\n    print(f"Message: {str(e)}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The client supports flexible model configuration for different prompt types:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'model_config = {\n    "general": "gpt-4o-mini",      # General purpose\n    "creative": "gpt-4o",          # Creative writing\n    "analysis": "gpt-4o",          # Text analysis\n    "title_generation": "gpt-4o-mini",  # Title generation\n    "character_generation": "gpt-4o",   # Character creation\n    "outline_generation": "gpt-4o",     # Outline creation\n    "chapter_writing": "gpt-4o",        # Chapter writing\n    "worldbuilding": "gpt-4o",          # World building\n    "review": "gpt-4o",                 # Content review\n}\n\nclient = LLMClient("openai", model_config=model_config)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"litellm-integration",children:"LiteLLM Integration"}),"\n",(0,i.jsx)(n.p,{children:"The client fully supports LiteLLM proxy with metadata tagging:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'client = LLMClient(\n    provider="openai",\n    environment="production",\n    project_name="my-book-project",\n    user="author@example.com"\n)\n\n# Headers automatically include:\n# X-Litellm-Tag-Environment: production\n# X-Litellm-Tag-Project: my-book-project\n# X-Litellm-Tag-User: author@example.com\n'})}),"\n",(0,i.jsx)(n.h2,{id:"content-filtering-analysis",children:"Content Filtering Analysis"}),"\n",(0,i.jsx)(n.p,{children:"The client includes sophisticated content filtering analysis:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def _analyze_content_filtering_triggers(self, prompt: Any) -> list[str]\n"})}),"\n",(0,i.jsx)(n.p,{children:"Detects potential triggers:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Unicode escape sequences"}),"\n",(0,i.jsx)(n.li,{children:"HTML-like tags"}),"\n",(0,i.jsx)(n.li,{children:"JavaScript injection patterns"}),"\n",(0,i.jsx)(n.li,{children:"URLs and email patterns"}),"\n",(0,i.jsx)(n.li,{children:"IP addresses"}),"\n",(0,i.jsx)(n.li,{children:"Non-ASCII characters"}),"\n",(0,i.jsx)(n.li,{children:"Very long prompts"}),"\n",(0,i.jsx)(n.li,{children:"Repeated patterns"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"provider-specific-implementation",children:"Provider-Specific Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"openai-provider",children:"OpenAI Provider"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Full OpenAI API compatibility"}),"\n",(0,i.jsx)(n.li,{children:"LiteLLM proxy support"}),"\n",(0,i.jsx)(n.li,{children:"Comprehensive error handling"}),"\n",(0,i.jsx)(n.li,{children:"Content filtering detection"}),"\n",(0,i.jsx)(n.li,{children:"Request/response validation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"mock-provider",children:"Mock Provider"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Deterministic responses for testing"}),"\n",(0,i.jsx)(n.li,{children:"Language-specific content generation"}),"\n",(0,i.jsx)(n.li,{children:"Configurable content length"}),"\n",(0,i.jsx)(n.li,{children:"JSON response simulation"}),"\n",(0,i.jsx)(n.li,{children:"No API costs"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,i.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from libriscribe2.utils.llm_client import LLMClient\n\n# Initialize client\nclient = LLMClient(\n    provider="openai",\n    model_config={"creative": "gpt-4o", "general": "gpt-4o-mini"},\n    timeout=300.0,\n    project_name="my-book"\n)\n\n# Generate content\ncontent = await client.generate_content(\n    "Write a compelling book opening",\n    prompt_type="creative",\n    temperature=0.8\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-usage-with-error-handling",children:"Advanced Usage with Error Handling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from libriscribe2.utils.llm_client import LLMClient, LLMClientError\n\nclient = LLMClient("openai")\n\ntry:\n    # Try with content filtering fallback\n    content = await client.generate_content_with_content_filtering_fallback(\n        primary_prompt="Complex prompt that might trigger filtering",\n        fallback_prompt="Simple alternative prompt",\n        prompt_type="creative",\n        max_retries=3\n    )\n\n    if content:\n        print(f"Generated: {content}")\n    else:\n        print("All generation attempts failed")\n\nexcept LLMClientError as e:\n    print(f"Error from {e.provider}: {e}")\n    if e.context:\n        print(f"Context: {e.context}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"streaming-generation",children:"Streaming Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'print("Generating story...")\nasync for chunk in client.generate_streaming_content(\n    "Tell me an adventure story",\n    temperature=0.7\n):\n    print(chunk, end="", flush=True)\nprint("\\nDone!")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'content, duration = await client.generate_content_with_timing(\n    "Analyze this text for themes and motifs"\n)\n\nprint(f"Analysis completed in {duration:.2f} seconds")\nprint(f"Generated {len(content)} characters")\nprint(f"Rate: {len(content)/duration:.1f} chars/second")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"context-manager-usage",children:"Context Manager Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async with LLMClient("openai", project_name="my-book") as client:\n    # Generate multiple pieces of content\n    title = await client.generate_content("Generate a book title", "title_generation")\n    outline = await client.generate_content("Create a chapter outline", "outline_generation")\n\n    # Client automatically cleaned up\n'})}),"\n",(0,i.jsx)(n.h2,{id:"configuration-integration",children:"Configuration Integration"}),"\n",(0,i.jsx)(n.p,{children:"The LLMClient integrates with LibriScribe2's settings system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from libriscribe2.settings import Settings\n\nsettings = Settings()\nclient = LLMClient(\n    provider=settings.default_llm,\n    timeout=settings.llm_timeout,\n    environment=settings.environment\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use appropriate prompt types"})," for optimal model selection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handle LLMClientError exceptions"})," with proper error context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use content filtering fallback"})," for prompts that might trigger filtering"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor performance"})," with timing methods for optimization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use context managers"})," for proper resource cleanup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configure models per prompt type"})," for cost optimization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Set appropriate timeouts"})," based on expected generation time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use mock provider"})," for testing to avoid API costs"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"thread-safety",children:"Thread Safety"}),"\n",(0,i.jsx)(n.p,{children:"The LLMClient is designed to be thread-safe for concurrent usage:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No shared mutable state between requests"}),"\n",(0,i.jsx)(n.li,{children:"Each request uses independent HTTP sessions"}),"\n",(0,i.jsx)(n.li,{children:"Error handling is request-specific"}),"\n",(0,i.jsx)(n.li,{children:"Logging is thread-safe"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Selection"}),": Use smaller models (gpt-4o-mini) for simple tasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timeout Configuration"}),": Set appropriate timeouts based on content length"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Filtering"}),": Use fallback methods to avoid retry delays"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Streaming"}),": Use streaming for long-form content generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Connection Pooling"}),": HTTP sessions are automatically managed"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"migration-guide",children:"Migration Guide"}),"\n",(0,i.jsx)(n.h3,{id:"from-previous-versions",children:"From Previous Versions"}),"\n",(0,i.jsx)(n.p,{children:"The new LLMClient includes breaking changes:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Constructor changes"}),": New parameters for environment and project metadata"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error handling"}),": New LLMClientError with enhanced context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model configuration"}),": Per-prompt-type model selection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Async context manager"}),": New resource management patterns"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"updating-code",children:"Updating Code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Old usage\nclient = LLMClient("openai")\ncontent = await client.generate_content("prompt")\n\n# New usage\nclient = LLMClient(\n    provider="openai",\n    model_config={"general": "gpt-4o-mini"},\n    project_name="my-project"\n)\ncontent = await client.generate_content("prompt", prompt_type="general")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/libriscribe2/docs/user-guide/configuration",children:"Settings Configuration"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/libriscribe2/docs/api/mock-llm-client-api",children:"Mock LLM Client"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(6540);const i={},l=r.createContext(i);function s(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);