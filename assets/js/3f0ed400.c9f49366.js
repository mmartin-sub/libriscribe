"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[169],{1322:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"api/llm-client-signatures","title":"LLM Client - Function and Class Signatures","description":"Overview","source":"@site/docs/api/llm-client-signatures.md","sourceDirName":"api","slug":"/api/llm-client-signatures","permalink":"/libriscribe2/docs/api/llm-client-signatures","draft":false,"unlisted":false,"editUrl":"https://github.com/guerra2fernando/libriscribe2/tree/main/docs/docs/api/llm-client-signatures.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Client API Documentation","permalink":"/libriscribe2/docs/api/llm-client-api"},"next":{"title":"Mock LLM Client API Documentation","permalink":"/libriscribe2/docs/api/mock-llm-client-api"}}');var i=t(4848),o=t(8453);const a={},s="LLM Client - Function and Class Signatures",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Type Aliases",id:"type-aliases",level:2},{value:"Protocols",id:"protocols",level:2},{value:"LLMProviderProtocol",id:"llmproviderprotocol",level:3},{value:"Exception Classes",id:"exception-classes",level:2},{value:"LLMClientError",id:"llmclienterror",level:3},{value:"Main Classes",id:"main-classes",level:2},{value:"LLMClient",id:"llmclient",level:3},{value:"Public Methods",id:"public-methods",level:4},{value:"Core Generation Methods",id:"core-generation-methods",level:5},{value:"Utility Methods",id:"utility-methods",level:5},{value:"Context Manager Methods",id:"context-manager-methods",level:5},{value:"Private Methods",id:"private-methods",level:4},{value:"Configuration and Validation",id:"configuration-and-validation",level:5},{value:"Provider-Specific Implementation",id:"provider-specific-implementation",level:5},{value:"Constants and Defaults",id:"constants-and-defaults",level:2},{value:"Usage Patterns",id:"usage-patterns",level:2},{value:"Basic Initialization",id:"basic-initialization",level:3},{value:"Content Generation Patterns",id:"content-generation-patterns",level:3},{value:"Streaming Generation Patterns",id:"streaming-generation-patterns",level:3},{value:"Context Manager Patterns",id:"context-manager-patterns",level:3},{value:"Error Handling Patterns",id:"error-handling-patterns",level:3},{value:"Configuration Patterns",id:"configuration-patterns",level:3},{value:"Return Type Specifications",id:"return-type-specifications",level:2},{value:"Method Return Types",id:"method-return-types",level:3},{value:"Exception Types",id:"exception-types",level:3},{value:"Parameter Type Specifications",id:"parameter-type-specifications",level:2},{value:"Common Parameter Types",id:"common-parameter-types",level:3},{value:"Model Configuration Structure",id:"model-configuration-structure",level:3},{value:"Import Requirements",id:"import-requirements",level:2},{value:"Complete Method Signature Summary",id:"complete-method-signature-summary",level:2},{value:"See Also",id:"see-also",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llm-client---function-and-class-signatures",children:"LLM Client - Function and Class Signatures"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:["This document provides a comprehensive reference of all function and class signatures in the ",(0,i.jsx)(n.code,{children:"src/libriscribe2/utils/llm_client.py"})," module."]}),"\n",(0,i.jsx)(n.h2,{id:"type-aliases",children:"Type Aliases"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"ModelType = str\nPromptType = str\nResponseType = str | dict[str, Any]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"protocols",children:"Protocols"}),"\n",(0,i.jsx)(n.h3,{id:"llmproviderprotocol",children:"LLMProviderProtocol"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LLMProviderProtocol(Protocol):\n    """Protocol for LLM providers using Python 3.12 features."""\n\n    async def generate_content(self, prompt: str, **kwargs: Any) -> str:\n        """Generate content from a prompt."""\n        ...\n\n    async def generate_streaming_content(self, prompt: str, **kwargs: Any) -> AsyncIterator[str]:\n        """Generate streaming content from a prompt."""\n        ...\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exception-classes",children:"Exception Classes"}),"\n",(0,i.jsx)(n.h3,{id:"llmclienterror",children:"LLMClientError"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LLMClientError(Exception):\n    """Exception for LLM client errors with improved error messages."""\n\n    def __init__(\n        self,\n        message: str,\n        provider: str,\n        context: dict[str, Any] | None = None\n    ) -> None:\n        """Initialize LLM client error with context."""\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Attributes:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"provider"})," (str): The LLM provider that caused the error"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"context"})," (dict[str, Any] | None): Additional error context"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"main-classes",children:"Main Classes"}),"\n",(0,i.jsx)(n.h3,{id:"llmclient",children:"LLMClient"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LLMClient:\n    """LLM Client using Python 3.12 features."""\n\n    def __init__(\n        self,\n        provider: str,\n        model_config: dict[str, str] | None = None,\n        timeout: float = DEFAULT_TIMEOUT,\n        environment: str = DEFAULT_ENVIRONMENT,\n        project_name: str = "",\n        user: str | None = None,\n    ) -> None:\n        """Initialize LLM client with configuration."""\n'})}),"\n",(0,i.jsx)(n.h4,{id:"public-methods",children:"Public Methods"}),"\n",(0,i.jsx)(n.h5,{id:"core-generation-methods",children:"Core Generation Methods"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def generate_content(\n    self,\n    prompt: str,\n    prompt_type: str = "general",\n    temperature: float = DEFAULT_TEMPERATURE,\n    max_tokens: int | None = None,\n    **kwargs: Any,\n) -> str:\n    """Generate content with improved error handling and timeout."""\n\nasync def generate_streaming_content(\n    self,\n    prompt: str,\n    temperature: float = DEFAULT_TEMPERATURE,\n    max_tokens: int | None = None,\n    **kwargs: Any\n) -> AsyncIterator[str]:\n    """Generate streaming content with improved async iteration."""\n\nasync def generate_content_with_timing(\n    self,\n    prompt: str,\n    **kwargs: Any\n) -> tuple[str, float]:\n    """Generate content with timing information."""\n\nasync def generate_content_with_fallback(\n    self,\n    primary_prompt: str,\n    fallback_prompt: str | None = None,\n    **kwargs: Any\n) -> str:\n    """Generate content with fallback support."""\n\nasync def generate_content_with_content_filtering_fallback(\n    self,\n    primary_prompt: str,\n    fallback_prompt: str | None = None,\n    prompt_type: str = "general",\n    temperature: float = DEFAULT_TEMPERATURE,\n    max_retries: int = 2,\n    **kwargs: Any,\n) -> str | None:\n    """Generate content with fallback for content filtering issues."""\n'})}),"\n",(0,i.jsx)(n.h5,{id:"utility-methods",children:"Utility Methods"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def get_model_for_prompt_type(self, prompt_type: str) -> str:\n    """Gets the specific model for a given prompt type, falling back to default."""\n\ndef validate_prompt(self, prompt: Any) -> bool:\n    """Validate prompt using Python 3.12 features."""\n\ndef get_client_config(self) -> dict[str, Any]:\n    """Get client configuration using Python 3.12 features."""\n'})}),"\n",(0,i.jsx)(n.h5,{id:"context-manager-methods",children:"Context Manager Methods"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'@asynccontextmanager\nasync def client_session(self):\n    """Async context manager for LLM client sessions."""\n\nasync def __aenter__(self):\n    """Async context manager entry."""\n\nasync def __aexit__(self, exc_type, _exc_val, _exc_tb):\n    """Async context manager exit."""\n\nasync def initialize_session(self) -> None:\n    """Initialize client session."""\n\nasync def cleanup_session(self) -> None:\n    """Cleanup client session."""\n'})}),"\n",(0,i.jsx)(n.h4,{id:"private-methods",children:"Private Methods"}),"\n",(0,i.jsx)(n.h5,{id:"configuration-and-validation",children:"Configuration and Validation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def _validate_configuration(self) -> None:\n    """Validate client configuration using Python 3.12 features."""\n\ndef _analyze_content_filtering_triggers(self, prompt: Any) -> list[str]:\n    """Analyze prompt for potential content filtering triggers."""\n\ndef _create_simplified_prompt(self, original_prompt: str, prompt_type: str) -> str:\n    """Create a simplified version of the prompt to avoid content filtering."""\n'})}),"\n",(0,i.jsx)(n.h5,{id:"provider-specific-implementation",children:"Provider-Specific Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def _generate_openai_content(\n    self,\n    prompt: str,\n    temperature: float,\n    max_tokens: int | None,\n    model: str = OPENAI_DEFAULT_MODEL,\n    **kwargs: Any\n) -> str:\n    """Generate content using OpenAI."""\n\nasync def _generate_mock_content(\n    self,\n    prompt: str,\n    temperature: float,\n    **kwargs: Any\n) -> str:\n    """Generate mock content for testing."""\n\nasync def _generate_openai_streaming(\n    self,\n    prompt: str,\n    temperature: float,\n    max_tokens: int | None,\n    **kwargs: Any\n) -> AsyncIterator[str]:\n    """Generate streaming content using OpenAI."""\n\nasync def _generate_mock_streaming(\n    self,\n    prompt: str,\n    temperature: float,\n    **kwargs: Any\n) -> AsyncIterator[str]:\n    """Generate mock streaming content for testing."""\n'})}),"\n",(0,i.jsx)(n.h2,{id:"constants-and-defaults",children:"Constants and Defaults"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Type aliases\nT = TypeVar("T")\nModelType = str\nPromptType = str\nResponseType = str | dict[str, Any]\n\n# Default values from settings\nDEFAULT_TIMEOUT: float = 300.0\nDEFAULT_TEMPERATURE: float = 0.7\nDEFAULT_MAX_TOKENS: int = 100000\nDEFAULT_ENVIRONMENT: str = "production"\nFALLBACK_MODEL: str = "gpt-4o-mini"\nOPENAI_DEFAULT_MODEL: str = "gpt-4o-mini"\nOPENAI_BASE_URL: str = "https://api.openai.com/v1"\nCLIENT_VERSION: str = "1.0.0"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"usage-patterns",children:"Usage Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"basic-initialization",children:"Basic Initialization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Minimal initialization\nclient = LLMClient("openai")\n\n# Full initialization\nclient = LLMClient(\n    provider="openai",\n    model_config={"creative": "gpt-4o", "general": "gpt-4o-mini"},\n    timeout=300.0,\n    environment="production",\n    project_name="my-book-project",\n    user="author@example.com"\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"content-generation-patterns",children:"Content Generation Patterns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Basic generation\ncontent: str = await client.generate_content("Write a story")\n\n# With prompt type\ncontent: str = await client.generate_content(\n    "Write a creative story",\n    prompt_type="creative"\n)\n\n# With all parameters\ncontent: str = await client.generate_content(\n    prompt="Write a story",\n    prompt_type="creative",\n    temperature=0.8,\n    max_tokens=1000\n)\n\n# With timing\ncontent: str\nduration: float\ncontent, duration = await client.generate_content_with_timing("Write a story")\n\n# With fallback\ncontent: str = await client.generate_content_with_fallback(\n    primary_prompt="Complex prompt",\n    fallback_prompt="Simple prompt"\n)\n\n# With content filtering fallback\ncontent: str | None = await client.generate_content_with_content_filtering_fallback(\n    primary_prompt="Potentially filtered prompt",\n    fallback_prompt="Safe prompt",\n    max_retries=3\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"streaming-generation-patterns",children:"Streaming Generation Patterns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Basic streaming\nasync for chunk in client.generate_streaming_content("Tell a story"):\n    chunk: str\n    print(chunk, end="")\n\n# Streaming with parameters\nasync for chunk in client.generate_streaming_content(\n    prompt="Tell a story",\n    temperature=0.7,\n    max_tokens=1000\n):\n    chunk: str\n    process_chunk(chunk)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"context-manager-patterns",children:"Context Manager Patterns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Session context manager\nasync with client.client_session():\n    content = await client.generate_content("Write content")\n\n# Full context manager\nasync with LLMClient("openai") as client:\n    content = await client.generate_content("Write content")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"error-handling-patterns",children:"Error Handling Patterns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Basic error handling\ntry:\n    content = await client.generate_content("Write content")\nexcept LLMClientError as e:\n    print(f"Provider: {e.provider}")\n    print(f"Context: {e.context}")\n    print(f"Error: {str(e)}")\n\n# Specific error handling\ntry:\n    content = await client.generate_content("Write content")\nexcept LLMClientError as e:\n    if "timeout" in str(e).lower():\n        # Handle timeout\n        pass\n    elif "content filtering" in str(e).lower():\n        # Handle content filtering\n        pass\n    else:\n        # Handle other errors\n        pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"configuration-patterns",children:"Configuration Patterns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Get model for prompt type\nmodel: str = client.get_model_for_prompt_type("creative")\n\n# Validate prompt\nis_valid: bool = client.validate_prompt("Some prompt text")\n\n# Get client configuration\nconfig: dict[str, Any] = client.get_client_config()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"return-type-specifications",children:"Return Type Specifications"}),"\n",(0,i.jsx)(n.h3,{id:"method-return-types",children:"Method Return Types"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Synchronous methods\nget_model_for_prompt_type(str) -> str\nvalidate_prompt(Any) -> bool\nget_client_config() -> dict[str, Any]\n\n# Asynchronous methods\ngenerate_content(...) -> str\ngenerate_streaming_content(...) -> AsyncIterator[str]\ngenerate_content_with_timing(...) -> tuple[str, float]\ngenerate_content_with_fallback(...) -> str\ngenerate_content_with_content_filtering_fallback(...) -> str | None\ninitialize_session() -> None\ncleanup_session() -> None\n\n# Context manager methods\n__aenter__() -> LLMClient\n__aexit__(...) -> None\nclient_session() -> AsyncContextManager[LLMClient]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"exception-types",children:"Exception Types"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Raised exceptions\nLLMClientError: Exception with provider and context information\nTimeoutError: When operations exceed timeout\nValueError: For invalid configuration or parameters\n"})}),"\n",(0,i.jsx)(n.h2,{id:"parameter-type-specifications",children:"Parameter Type Specifications"}),"\n",(0,i.jsx)(n.h3,{id:"common-parameter-types",children:"Common Parameter Types"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# String parameters\nprovider: str                    # Required, non-empty\nprompt: str                     # Required, non-empty\nprompt_type: str               # Optional, defaults to "general"\nenvironment: str               # Optional, defaults to "production"\nproject_name: str              # Optional, defaults to ""\nmodel: str                     # Optional, provider-specific default\n\n# Numeric parameters\ntemperature: float             # 0.0 to 1.0, defaults to 0.7\ntimeout: float                 # Positive number, defaults to 300.0\nmax_tokens: int | None         # Positive integer or None\nmax_retries: int              # Non-negative integer, defaults to 2\n\n# Optional parameters\nuser: str | None              # Optional user identifier\nfallback_prompt: str | None   # Optional fallback prompt\nmodel_config: dict[str, str] | None  # Optional model configuration\ncontext: dict[str, Any] | None       # Optional error context\n\n# Complex parameters\n**kwargs: Any                 # Provider-specific additional parameters\n'})}),"\n",(0,i.jsx)(n.h3,{id:"model-configuration-structure",children:"Model Configuration Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'model_config: dict[str, str] = {\n    "general": "gpt-4o-mini",\n    "creative": "gpt-4o",\n    "analysis": "gpt-4o",\n    "title_generation": "gpt-4o-mini",\n    "character_generation": "gpt-4o",\n    "outline_generation": "gpt-4o",\n    "chapter_writing": "gpt-4o",\n    "worldbuilding": "gpt-4o",\n    "review": "gpt-4o",\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"import-requirements",children:"Import Requirements"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import asyncio\nimport logging\nimport os\nimport re\nimport time\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\nfrom typing import Any, Protocol, TypeVar\n\nimport aiohttp\n\nfrom ..settings import (\n    CLIENT_VERSION,\n    DEFAULT_ENVIRONMENT,\n    DEFAULT_MAX_TOKENS,\n    DEFAULT_MODEL_CONFIG,\n    DEFAULT_TEMPERATURE,\n    DEFAULT_TIMEOUT,\n    FALLBACK_MODEL,\n    OPENAI_BASE_URL,\n    OPENAI_DEFAULT_MODEL,\n)\nfrom .validation_mixin import ValidationMixin\n"})}),"\n",(0,i.jsx)(n.h2,{id:"complete-method-signature-summary",children:"Complete Method Signature Summary"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LLMClient:\n    # Constructor\n    def __init__(self, provider: str, model_config: dict[str, str] | None = None,\n                 timeout: float = DEFAULT_TIMEOUT, environment: str = DEFAULT_ENVIRONMENT,\n                 project_name: str = "", user: str | None = None) -> None\n\n    # Core generation methods\n    async def generate_content(self, prompt: str, prompt_type: str = "general",\n                              temperature: float = DEFAULT_TEMPERATURE,\n                              max_tokens: int | None = None, **kwargs: Any) -> str\n\n    async def generate_streaming_content(self, prompt: str,\n                                        temperature: float = DEFAULT_TEMPERATURE,\n                                        max_tokens: int | None = None,\n                                        **kwargs: Any) -> AsyncIterator[str]\n\n    async def generate_content_with_timing(self, prompt: str, **kwargs: Any) -> tuple[str, float]\n\n    async def generate_content_with_fallback(self, primary_prompt: str,\n                                            fallback_prompt: str | None = None,\n                                            **kwargs: Any) -> str\n\n    async def generate_content_with_content_filtering_fallback(\n        self, primary_prompt: str, fallback_prompt: str | None = None,\n        prompt_type: str = "general", temperature: float = DEFAULT_TEMPERATURE,\n        max_retries: int = 2, **kwargs: Any) -> str | None\n\n    # Utility methods\n    def get_model_for_prompt_type(self, prompt_type: str) -> str\n    def validate_prompt(self, prompt: Any) -> bool\n    def get_client_config(self) -> dict[str, Any]\n\n    # Context manager methods\n    async def __aenter__(self) -> LLMClient\n    async def __aexit__(self, exc_type, _exc_val, _exc_tb) -> None\n    async def initialize_session(self) -> None\n    async def cleanup_session(self) -> None\n\n    @asynccontextmanager\n    async def client_session(self) -> AsyncIterator[LLMClient]\n\n    # Private methods\n    def _validate_configuration(self) -> None\n    def _analyze_content_filtering_triggers(self, prompt: Any) -> list[str]\n    def _create_simplified_prompt(self, original_prompt: str, prompt_type: str) -> str\n\n    async def _generate_openai_content(self, prompt: str, temperature: float,\n                                      max_tokens: int | None,\n                                      model: str = OPENAI_DEFAULT_MODEL,\n                                      **kwargs: Any) -> str\n\n    async def _generate_mock_content(self, prompt: str, temperature: float,\n                                    **kwargs: Any) -> str\n\n    async def _generate_openai_streaming(self, prompt: str, temperature: float,\n                                        max_tokens: int | None,\n                                        **kwargs: Any) -> AsyncIterator[str]\n\n    async def _generate_mock_streaming(self, prompt: str, temperature: float,\n                                      **kwargs: Any) -> AsyncIterator[str]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/libriscribe2/docs/api/llm-client-api",children:"LLM Client API Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/libriscribe2/docs/user-guide/configuration",children:"Settings Configuration"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var r=t(6540);const i={},o=r.createContext(i);function a(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);